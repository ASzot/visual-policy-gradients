2ee9fb9477e4631481034b93fd5308cdf6f3723f
diff --git a/reinforce.py b/reinforce.py
index cfa0292..0aaaa2b 100644
--- a/reinforce.py
+++ b/reinforce.py
@@ -1,12 +1,15 @@
+from dataclasses import dataclass
+
 import numpy as np
 import torch
 import torch.nn as nn
 from einops import reduce
 from rlf.envs.pointmass import LinearPointMassEnv
+from rlf.rl.loggers import PltLogger
 from torch.distributions import Normal
 from tqdm import tqdm
 
-num_envs = 128
+num_envs = 2
 H = 5
 lr = 0.01
 n_updates = 100
@@ -15,26 +18,28 @@ state_dim = 2
 action_dim = 1
 
 
+class LoggerArgs:
+    log_smooth_len = 10
+    prefix = "test"
+    log_dir = "data/log"
+    env_name = "LinearPm"
+    vid_dir: "data/vids"
+    seed = 31
+
+
 env = LinearPointMassEnv(num_envs)
 
 
 class Policy(nn.Module):
     def __init__(self):
         super().__init__()
-        self.weight = nn.Parameter(0.1 * torch.randn(state_dim, 1))
+        # self.weight = nn.Parameter(0.1 * torch.randn(state_dim, 1))
+        self.weight = nn.Parameter(torch.tensor([1.0, 0.0]).view(state_dim, 1))
         # self.weight = nn.Parameter(torch.zeros(state_dim, 1))
         self.logstd = nn.Parameter(torch.zeros(1, action_dim))
-        self.net = nn.Sequential(
-            nn.Linear(state_dim, 16),
-            nn.ReLU(),
-            nn.Linear(16, 16),
-            nn.ReLU(),
-            nn.Linear(16, action_dim),
-        )
 
     def forward(self, state):
-        # mean = state @ self.weight
-        mean = self.net(state)
+        mean = state @ self.weight
         logstd = self.logstd.expand_as(mean)
         return Normal(mean, logstd.exp())
 
@@ -49,6 +54,9 @@ def calculate_returns(rollout_returns, rollout_rewards):
             )
 
 
+logger = PltLogger()
+logger.init(LoggerArgs())
+
 policy = Policy()
 
 opt = torch.optim.SGD(policy.parameters(), lr=lr)
@@ -62,13 +70,15 @@ for update_i in range(n_updates):
     obs = env.reset()
     rollout_obs[0].copy_(obs)
     all_ep_rewards = []
-    print("")
+    logger.start_interval_log()
 
     with torch.no_grad():
         for i in range(H):
             pi = policy(obs)
-            action = pi.sample()
+            # action = pi.sample()
+            action = pi.mean
             obs, reward, done, info = env.step(action)
+            logger.collect_step_info(info)
 
             rollout_obs[i].copy_(obs)
             rollout_actions[i].copy_(action)
@@ -78,8 +88,6 @@ for update_i in range(n_updates):
         avg_ep_reward = reduce(rollout_rewards, "h n 1 -> n", "sum").mean()
         avg_dist_to_goal = np.mean([x["ep_dist_to_goal"] for x in info])
 
-        print(avg_ep_reward)
-        print(avg_dist_to_goal)
         all_ep_rewards.append(avg_ep_reward)
 
     opt.zero_grad()
@@ -90,4 +98,18 @@ for update_i in range(n_updates):
     J.backward()
 
     opt.step()
-    print(policy.weight, policy.logstd.exp())
+
+    logger.interval_log(
+        update_i,
+        update_i * H * num_envs,
+        0,
+        {
+            "loss": J.item(),
+            "pi_w1": policy.weight[0].item(),
+            "pi_w2": policy.weight[1].item(),
+            "pi_std": policy.logstd.exp().item(),
+        },
+        False,
+    )
+
+logger.close()
