{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f598855a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from rl_helper.envs import create_vectorized_envs\n",
    "from rl_helper.envs.pointmass.pointmass_env import PointMassParams\n",
    "from torch.distributions import Normal\n",
    "use_params = PointMassParams(clip_actions=True, radius=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7777ef6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        noise_scale = 1.0\n",
    "        self.weight = nn.Parameter(noise_scale * torch.randn(2))\n",
    "        self.logstd = nn.Parameter(torch.zeros(1, 1))\n",
    "\n",
    "    def forward(self, state):\n",
    "        mean = state * self.weight\n",
    "        logstd = self.logstd.expand_as(mean)\n",
    "        return Normal(mean, logstd.exp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca21287a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(num_eval_episodes, policy, envs):\n",
    "    obs = envs.reset()\n",
    "    total_rewards = []\n",
    "    while len(total_rewards) < num_eval_episodes:\n",
    "        action_distrib = policy(obs)\n",
    "        use_action = action_distrib.mean\n",
    "        obs, _, done, info = envs.step(use_action)\n",
    "        for done_i in torch.nonzero(done):\n",
    "            # dists_to_goal.append(info[done_i][\"dist_to_goal\"])\n",
    "            total_rewards.append(info[done_i][\"episode\"][\"r\"])\n",
    "    return sum(total_rewards) / len(total_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25c9b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_true_performance():\n",
    "    grid_density = 20\n",
    "    min_range = -2.0\n",
    "    max_range = 2.0\n",
    "\n",
    "    policy_values = torch.zeros(grid_density, grid_density)\n",
    "    weight_X = torch.linspace(min_range, max_range, grid_density)\n",
    "    weight_Y = torch.linspace(min_range, max_range, grid_density)\n",
    "    policy = Policy()\n",
    "    # Grid over [-2,0]^2\n",
    "    for i, weight_x in enumerate(weight_X):\n",
    "        for j, weight_y in enumerate(weight_Y):\n",
    "            policy.weight.data.copy_(torch.tensor([weight_x, weight_y]))\n",
    "            policy_values[i, j] = evaluate(1, policy, envs)\n",
    "\n",
    "    fig = plt.imshow(\n",
    "        policy_values,\n",
    "        extent=[min_range, max_range, min_range, max_range],\n",
    "        origin=\"lower\",\n",
    "    )\n",
    "    print(\"Maximum possible reward is \", policy_values.max())\n",
    "\n",
    "\n",
    "plot_true_performance()\n",
    "plt.colorbar()\n",
    "plt.savefig(\"data/perf_gt.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "918ca5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def compute_returns(rewards, masks, gamma):\n",
    "    returns = torch.zeros(rewards.shape[0] + 1, *rewards.shape[1:])\n",
    "    for step in reversed(range(rewards.size(0))):\n",
    "        returns[step] = returns[step + 1] * gamma * masks[step + 1] + rewards[step]\n",
    "    return returns\n",
    "\n",
    "\n",
    "def rollout_policy(policy, envs, num_steps):\n",
    "    all_obs = torch.zeros(num_steps + 1, envs.num_envs, envs.observation_space.shape[0])\n",
    "    all_rewards = torch.zeros(num_steps, envs.num_envs, 1)\n",
    "    all_actions = torch.zeros(num_steps, envs.num_envs, envs.action_space.shape[0])\n",
    "    all_masks = torch.zeros(num_steps + 1, envs.num_envs, 1)\n",
    "\n",
    "    obs = envs.reset()\n",
    "    all_obs[0].copy_(obs)\n",
    "\n",
    "    for step_idx in range(num_steps):\n",
    "        with torch.no_grad():\n",
    "            action_distrib = policy(obs)\n",
    "            take_action = action_distrib.sample()\n",
    "\n",
    "        obs, reward, done, info = envs.step(take_action)\n",
    "        all_obs[step_idx + 1].copy_(obs)\n",
    "        all_rewards[step_idx].copy_(reward)\n",
    "        all_actions[step_idx].copy_(take_action)\n",
    "        all_masks[step_idx].copy_((~done).float().view(-1, 1))\n",
    "    return all_obs, all_rewards, all_actions, all_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0499088",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_steps = 5\n",
    "num_updates = 100\n",
    "num_envs = 256\n",
    "gamma = 0.99\n",
    "envs = create_vectorized_envs(\n",
    "    \"PointMass-v0\",\n",
    "    num_envs,\n",
    "    params=use_params,\n",
    ")\n",
    "policy = Policy()\n",
    "opt = torch.optim.Adam(lr=1e-2, params=policy.parameters())\n",
    "log_interval = 20\n",
    "\n",
    "weight_seq = [policy.weight.data.detach().clone()]\n",
    "for update_i in range(num_updates):\n",
    "    obs, rewards, actions, masks = rollout_policy(policy, envs, num_steps)\n",
    "    returns = compute_returns(rewards, masks, gamma)\n",
    "    log_probs = policy(obs[:-1]).log_prob(actions).sum(-1, keepdim=True)\n",
    "    loss = (-returns[:-1] * log_probs).mean()\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    weight_seq.append(policy.weight.detach().clone())\n",
    "\n",
    "    if update_i % log_interval == 0:\n",
    "        eval_envs = create_vectorized_envs(\n",
    "            \"PointMass-v0\",\n",
    "            num_envs,\n",
    "            params=use_params,\n",
    "        )\n",
    "        total_reward = evaluate(10, policy, eval_envs)\n",
    "        print(f\"Update #{update_i}: Reward {total_reward:.4f}\")\n",
    "\n",
    "weight_seq = torch.stack(weight_seq, dim=0)\n",
    "plot_true_performance()\n",
    "fig = plt.scatter(\n",
    "    weight_seq[:, 0],\n",
    "    weight_seq[:, 1],\n",
    "    c=torch.arange(weight_seq.size(0)),\n",
    "    s=4,\n",
    "    cmap=plt.get_cmap(\"Reds\"),\n",
    ")\n",
    "plt.savefig(\"data/perf_reinforce_opt.png\")\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861b7fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_density = 20\n",
    "min_range = -2.0\n",
    "max_range = 2.0\n",
    "\n",
    "weight_X = torch.linspace(min_range, max_range, grid_density)\n",
    "weight_Y = torch.linspace(min_range, max_range, grid_density)\n",
    "\n",
    "for num_envs in [1, 2, 4, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096]:\n",
    "    envs = create_vectorized_envs(\n",
    "        \"PointMass-v0\",\n",
    "        num_envs,\n",
    "        params=use_params,\n",
    "    )\n",
    "    policy_values = torch.zeros(grid_density, grid_density)\n",
    "    policy = Policy()\n",
    "    for i, weight_x in enumerate(weight_X):\n",
    "        for j, weight_y in enumerate(weight_Y):\n",
    "            policy.weight.data.copy_(torch.tensor([weight_x, weight_y]))\n",
    "            obs, rewards, actions, masks = rollout_policy(policy, envs, num_steps=5)\n",
    "            returns = compute_returns(rewards, masks, gamma=0.99)\n",
    "\n",
    "            log_probs = policy(obs[:-1]).log_prob(actions).sum(-1, keepdim=True)\n",
    "            loss = (-returns[:-1] * log_probs).mean()\n",
    "            policy_values[i, j] = loss.item()\n",
    "\n",
    "    fig = plt.imshow(\n",
    "        policy_values,\n",
    "        extent=[min_range, max_range, min_range, max_range],\n",
    "        origin=\"lower\",\n",
    "    )\n",
    "    plt.savefig(f\"data/perf_est_{num_envs}.png\")\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb8b135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-9-29d22ee4f21b>\u001b[0m(2)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m      1 \u001b[0;31m\u001b[0;31m# Compute the gradient accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m----> 2 \u001b[0;31m\u001b[0;32mfor\u001b[0m \u001b[0mnum_envs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4096\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      3 \u001b[0;31m    envs = create_vectorized_envs(\n",
      "\u001b[0m\u001b[0;32m      4 \u001b[0;31m        \u001b[0;34m\"PointMass-v0\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      5 \u001b[0;31m        \u001b[0mnum_envs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> l\n",
      "\u001b[1;32m      1 \u001b[0m\u001b[0;31m# Compute the gradient accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m----> 2 \u001b[0;31m\u001b[0;32mfor\u001b[0m \u001b[0mnum_envs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4096\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m      3 \u001b[0m    envs = create_vectorized_envs(\n",
      "\u001b[1;32m      4 \u001b[0m        \u001b[0;34m\"PointMass-v0\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m      5 \u001b[0m        \u001b[0mnum_envs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m      6 \u001b[0m        \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m      7 \u001b[0m    )\n",
      "\u001b[1;32m      8 \u001b[0m    \u001b[0mpolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m      9 \u001b[0m    \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrollout_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menvs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     10 \u001b[0m    \u001b[0mreturns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_returns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     11 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "ipdb> policy.weights\n",
      "*** AttributeError: 'Policy' object has no attribute 'weights'\n",
      "ipdb> policy.weight\n",
      "Parameter containing:\n",
      "tensor([1.1108, 1.9839], requires_grad=True)\n",
      "ipdb> policy.weight.grad\n",
      "tensor([ 0.2560, -0.0129])\n"
     ]
    }
   ],
   "source": [
    "# Compute the gradient accuracy\n",
    "for num_envs in [1, 2, 4, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096]:\n",
    "    envs = create_vectorized_envs(\n",
    "        \"PointMass-v0\",\n",
    "        num_envs,\n",
    "        params=use_params,\n",
    "    )\n",
    "    policy = Policy()\n",
    "    obs, rewards, actions, masks = rollout_policy(policy, envs, num_steps=5)\n",
    "    returns = compute_returns(rewards, masks, gamma=0.99)\n",
    "\n",
    "    log_probs = policy(obs[:-1]).log_prob(actions).sum(-1, keepdim=True)\n",
    "    loss = (-returns[:-1] * log_probs).mean()\n",
    "    loss.backward()\n",
    "    breakpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953d4733",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
