{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f598855a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from rl_helper.envs import create_vectorized_envs\n",
    "from rl_helper.envs.pointmass.pointmass_env import PointMassParams\n",
    "from torch.distributions import Normal\n",
    "\n",
    "plt.rcParams[\"text.usetex\"] = True\n",
    "AXIS_FONT_SIZE = 20\n",
    "POLICY_POINT_SIZE = 34\n",
    "MIN_POLICY_WEIGHT = -2.0\n",
    "MAX_POLICY_WEIGHT = 2.0\n",
    "GRID_DENSITY = 51\n",
    "NUM_POLICY_UPDATES = 100\n",
    "POLICY_TRAJ_CMAP = \"spring\"\n",
    "SAVE_KWARGS = {\"bbox_inches\": \"tight\", \"pad_inches\": 0.1}\n",
    "\n",
    "use_params = PointMassParams(clip_actions=True, radius=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7777ef6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, weight_scale=0.1):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(weight_scale * torch.randn(2))\n",
    "        self.logstd = nn.Parameter(torch.zeros(1, 1))\n",
    "\n",
    "    def forward(self, state):\n",
    "        mean = state * self.weight\n",
    "        logstd = self.logstd.expand_as(mean)\n",
    "        return Normal(mean, logstd.exp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca21287a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(num_eval_episodes, policy, envs):\n",
    "    obs = envs.reset()\n",
    "    total_rewards = []\n",
    "    while len(total_rewards) < num_eval_episodes:\n",
    "        action_distrib = policy(obs)\n",
    "        use_action = action_distrib.mean\n",
    "        obs, _, done, info = envs.step(use_action)\n",
    "        for done_i in torch.nonzero(done):\n",
    "            # dists_to_goal.append(info[done_i][\"dist_to_goal\"])\n",
    "            total_rewards.append(info[done_i][\"episode\"][\"r\"])\n",
    "    return sum(total_rewards) / len(total_rewards)\n",
    "\n",
    "def plot_true_performance(envs):\n",
    "    policy_values = torch.zeros(GRID_DENSITY, GRID_DENSITY)\n",
    "    weight_X = torch.linspace(MIN_POLICY_WEIGHT, MAX_POLICY_WEIGHT, GRID_DENSITY)\n",
    "    weight_Y = torch.linspace(MIN_POLICY_WEIGHT, MAX_POLICY_WEIGHT, GRID_DENSITY)\n",
    "    policy = Policy()\n",
    "    # Grid over [-2,0]^2\n",
    "    max_val = -10000\n",
    "    max_weight = None\n",
    "\n",
    "    for i, weight_x in enumerate(weight_X):\n",
    "        for j, weight_y in enumerate(weight_Y):\n",
    "            policy.weight.data.copy_(torch.tensor([weight_x, weight_y]))\n",
    "            policy_values[i, j] = evaluate(1, policy, envs)\n",
    "            if policy_values[i, j] > max_val:\n",
    "                max_val = policy_values[i, j]\n",
    "                max_weight = [weight_x, weight_y]\n",
    "\n",
    "    fig = plt.imshow(\n",
    "        policy_values,\n",
    "        extent=[\n",
    "            MIN_POLICY_WEIGHT,\n",
    "            MAX_POLICY_WEIGHT,\n",
    "            MIN_POLICY_WEIGHT,\n",
    "            MAX_POLICY_WEIGHT,\n",
    "        ],\n",
    "        origin=\"lower\",\n",
    "    )\n",
    "    plt.xlabel(\"$\\\\theta_1$\", fontsize=AXIS_FONT_SIZE)\n",
    "    plt.ylabel(\"$\\\\theta_2$\", fontsize=AXIS_FONT_SIZE)\n",
    "    print(f\"Best parameters {max_weight} with value {max_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25c9b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = create_vectorized_envs(\n",
    "    \"PointMass-v0\",\n",
    "    32,\n",
    "    params=use_params,\n",
    ")\n",
    "plot_true_performance(envs)\n",
    "plt.colorbar()\n",
    "plt.savefig(\"data/perf_gt.png\", **SAVE_KWARGS)\n",
    "plt.show(block=False)\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918ca5d8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def compute_returns(rewards, masks, gamma):\n",
    "    returns = torch.zeros(rewards.shape[0] + 1, *rewards.shape[1:])\n",
    "    for step in reversed(range(rewards.size(0))):\n",
    "        returns[step] = returns[step + 1] * gamma * masks[step + 1] + rewards[step]\n",
    "    return returns\n",
    "\n",
    "\n",
    "def rollout_policy(policy, envs, num_steps):\n",
    "    all_obs = torch.zeros(num_steps + 1, envs.num_envs, envs.observation_space.shape[0])\n",
    "    all_rewards = torch.zeros(num_steps, envs.num_envs, 1)\n",
    "    all_actions = torch.zeros(num_steps, envs.num_envs, envs.action_space.shape[0])\n",
    "    all_masks = torch.zeros(num_steps + 1, envs.num_envs, 1)\n",
    "\n",
    "    obs = envs.reset()\n",
    "    all_obs[0].copy_(obs)\n",
    "\n",
    "    for step_idx in range(num_steps):\n",
    "        with torch.no_grad():\n",
    "            action_distrib = policy(obs)\n",
    "            take_action = action_distrib.sample()\n",
    "\n",
    "        obs, reward, done, info = envs.step(take_action)\n",
    "        all_obs[step_idx + 1].copy_(obs)\n",
    "        all_rewards[step_idx].copy_(reward)\n",
    "        all_actions[step_idx].copy_(take_action)\n",
    "        all_masks[step_idx].copy_((~done).float().view(-1, 1))\n",
    "    return all_obs, all_rewards, all_actions, all_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0499088",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_policy(envs, num_steps, num_updates, gamma=0.99, lr=1e-2, policy=None):\n",
    "    if policy is None:\n",
    "        policy = Policy()\n",
    "    opt = torch.optim.Adam(lr=lr, params=policy.parameters())\n",
    "    log_interval = 20\n",
    "\n",
    "    weight_seq = [policy.weight.data.detach().clone()]\n",
    "    for update_i in range(num_updates):\n",
    "        obs, rewards, actions, masks = rollout_policy(policy, envs, num_steps)\n",
    "        returns = compute_returns(rewards, masks, gamma)\n",
    "        log_probs = policy(obs[:-1]).log_prob(actions).sum(-1, keepdim=True)\n",
    "        loss = (-returns[:-1] * log_probs).mean()\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        if policy.weight[0] < MIN_POLICY_WEIGHT or policy.weight[1] < MIN_POLICY_WEIGHT:\n",
    "            break\n",
    "        weight_seq.append(policy.weight.detach().clone())\n",
    "\n",
    "    weight_seq = torch.stack(weight_seq, dim=0)\n",
    "    return weight_seq\n",
    "\n",
    "\n",
    "envs = create_vectorized_envs(\n",
    "    \"PointMass-v0\",\n",
    "    256,\n",
    "    params=use_params,\n",
    ")\n",
    "\n",
    "weight_seq = train_policy(envs, num_steps=5, num_updates=NUM_POLICY_UPDATES)\n",
    "plot_true_performance(envs)\n",
    "fig = plt.scatter(\n",
    "    weight_seq[:, 0],\n",
    "    weight_seq[:, 1],\n",
    "    c=torch.arange(weight_seq.size(0)),\n",
    "    s=POLICY_POINT_SIZE,\n",
    "    cmap=plt.get_cmap(POLICY_TRAJ_CMAP),\n",
    ")\n",
    "plt.savefig(\"data/perf_reinforce_opt.png\", **SAVE_KWARGS)\n",
    "plt.show(block=False)\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861b7fe7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Compute the loss function estimate as a function of the batch size.\n",
    "weight_X = torch.linspace(MIN_POLICY_WEIGHT, MAX_POLICY_WEIGHT, GRID_DENSITY)\n",
    "weight_Y = torch.linspace(MIN_POLICY_WEIGHT, MAX_POLICY_WEIGHT, GRID_DENSITY)\n",
    "\n",
    "for num_envs in [1, 2, 4, 16, 32, 64, 128, 1024]:\n",
    "    envs = create_vectorized_envs(\n",
    "        \"PointMass-v0\",\n",
    "        num_envs,\n",
    "        params=use_params,\n",
    "    )\n",
    "    policy_values = torch.zeros(GRID_DENSITY, GRID_DENSITY)\n",
    "    policy = Policy()\n",
    "    for i, weight_x in enumerate(weight_X):\n",
    "        for j, weight_y in enumerate(weight_Y):\n",
    "            policy.weight.data.copy_(torch.tensor([weight_x, weight_y]))\n",
    "            obs, rewards, actions, masks = rollout_policy(policy, envs, num_steps=5)\n",
    "            returns = compute_returns(rewards, masks, gamma=0.99)\n",
    "\n",
    "            log_probs = policy(obs[:-1]).log_prob(actions).sum(-1, keepdim=True)\n",
    "            loss = (-returns[:-1] * log_probs).mean()\n",
    "            policy_values[i, j] = loss.item()\n",
    "\n",
    "    plt.clf()\n",
    "    fig = plt.imshow(\n",
    "        policy_values,\n",
    "        extent=[\n",
    "            MIN_POLICY_WEIGHT,\n",
    "            MAX_POLICY_WEIGHT,\n",
    "            MIN_POLICY_WEIGHT,\n",
    "            MAX_POLICY_WEIGHT,\n",
    "        ],\n",
    "        origin=\"lower\",\n",
    "    )\n",
    "    plt.xlabel(\"$\\\\theta_1$\", fontsize=AXIS_FONT_SIZE)\n",
    "    plt.ylabel(\"$\\\\theta_2$\", fontsize=AXIS_FONT_SIZE)\n",
    "    plt.savefig(f\"data/perf_est_{num_envs}.png\", **SAVE_KWARGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb8b135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the gradient accuracy/variance\n",
    "def get_loss(policy, envs):\n",
    "    obs, rewards, actions, masks = rollout_policy(policy, envs, num_steps=5)\n",
    "    returns = compute_returns(rewards, masks, gamma=0.99)\n",
    "    log_probs = policy(obs[:-1]).log_prob(actions).sum(-1, keepdim=True)\n",
    "    return (-returns[:-1] * log_probs).mean()\n",
    "\n",
    "\n",
    "def compute_grad_mean_vars(env_params, env_sizes):\n",
    "    policy = Policy()\n",
    "    envs = create_vectorized_envs(\n",
    "        \"PointMass-v0\",\n",
    "        10000,\n",
    "        params=env_params,\n",
    "    )\n",
    "    true_loss = get_loss(policy, envs)\n",
    "    policy.zero_grad()\n",
    "    true_loss.backward()\n",
    "    true_grad = policy.weight.grad.detach().clone()\n",
    "\n",
    "    cosine_dist = nn.CosineSimilarity(dim=-1)\n",
    "    dists = []\n",
    "    all_pairwise_dists = []\n",
    "    n_samples = 100\n",
    "\n",
    "    for num_envs in env_sizes:\n",
    "        envs = create_vectorized_envs(\n",
    "            \"PointMass-v0\",\n",
    "            num_envs,\n",
    "            params=use_params,\n",
    "        )\n",
    "\n",
    "        all_grads = []\n",
    "        for _ in range(n_samples):\n",
    "            policy.zero_grad()\n",
    "            loss = get_loss(policy, envs)\n",
    "            loss.backward()\n",
    "            all_grads.append(policy.weight.grad.detach().clone())\n",
    "        all_grads = torch.stack(all_grads, 0)\n",
    "\n",
    "        dists.append(cosine_dist(all_grads, true_grad).mean())\n",
    "        all_pairwise_dists.append(all_grads.std(0).mean())\n",
    "\n",
    "    return dists, all_pairwise_dists\n",
    "\n",
    "\n",
    "env_sizes = torch.tensor([1, 2, 4, 16, 32, 64, 128, 256, 512, 1024])\n",
    "grad_acc, grad_var = compute_grad_mean_vars(use_params, env_sizes)\n",
    "\n",
    "plt.plot(env_sizes * 5, grad_acc)\n",
    "plt.xscale(\"log\", base=2)\n",
    "plt.xlabel(\"Number of Trajectories\", fontsize=AXIS_FONT_SIZE)\n",
    "plt.ylabel(\"Cosine Distance to True Gradient\", fontsize=AXIS_FONT_SIZE)\n",
    "plt.savefig(f\"data/grad_accuracy.png\")\n",
    "plt.show(block=False)\n",
    "plt.clf()\n",
    "\n",
    "plt.plot(env_sizes * 5, grad_var)\n",
    "plt.xscale(\"log\", base=2)\n",
    "plt.xlabel(\"Number of Trajectories\", fontsize=AXIS_FONT_SIZE)\n",
    "plt.ylabel(\"Gradient Variance\", fontsize=AXIS_FONT_SIZE)\n",
    "plt.savefig(f\"data/grad_var.png\")\n",
    "plt.show(block=False)\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953d4733",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_reward(cur_dist, prev_dist):\n",
    "    reward = torch.zeros(cur_dist.shape)\n",
    "    reward[cur_dist < 0.2] = 1.0\n",
    "    return reward\n",
    "\n",
    "\n",
    "sparse_reward_params = PointMassParams(\n",
    "    clip_actions=True, radius=1.0, custom_reward=sparse_reward\n",
    ")\n",
    "\n",
    "envs = create_vectorized_envs(\n",
    "    \"PointMass-v0\",\n",
    "    32,\n",
    "    params=sparse_reward_params,\n",
    ")\n",
    "plot_true_performance(envs)\n",
    "plt.savefig(\"data/sparse_reward_perf.png\", **SAVE_KWARGS)\n",
    "plt.show(block=False)\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4c5e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_eval_results = []\n",
    "for seed_i in range(100):\n",
    "    weight_seq = train_policy(\n",
    "        envs,\n",
    "        num_steps=5,\n",
    "        num_updates=50,\n",
    "        lr=1e-1,\n",
    "        policy=Policy(),\n",
    "    )\n",
    "    eval_policy = Policy()\n",
    "    eval_policy.weight.data.copy_(weight_seq[-1])\n",
    "    all_eval_results.append(evaluate(5, eval_policy, envs))\n",
    "    if seed_i > 10:\n",
    "        continue\n",
    "    fig = plt.scatter(\n",
    "        weight_seq[:, 0],\n",
    "        weight_seq[:, 1],\n",
    "        c=torch.arange(weight_seq.size(0)),\n",
    "        s=POLICY_POINT_SIZE,\n",
    "        cmap=plt.get_cmap(POLICY_TRAJ_CMAP),\n",
    "    )\n",
    "print(\n",
    "    \"% that achieved goal\",\n",
    "    sum(1 for v in all_eval_results if v > 0) / len(all_eval_results),\n",
    ")\n",
    "plt.savefig(\"data/sparse_reward_opt.png\", **SAVE_KWARGS)\n",
    "plt.show(block=False)\n",
    "plt.clf()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python [conda env:pt_dev] *",
   "language": "python",
   "name": "conda-env-pt_dev-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
